ğŸŒŸ Gemini Chatbot â€” AI-Powered Conversational App

An interactive chatbot built using **Google Gemini API**, **Streamlit**, and **Python**.
This project demonstrates how to integrate LLMs into a real-time web application with clean modular architecture.

---

## ğŸš€ Features

âœ” **Conversational AI** using Google Gemini
âœ” **Streamlit UI** for real-time chatting
âœ” **Modular Code Structure** (`chatbot.py` for model logic, `app.py` for UI)
âœ” **Ngrok Integration** to run the app publicly from Google Colab

---

## ğŸ“ Project Structure

```
my_chatbot_project/
â”‚
â”œâ”€â”€ chatbot.py        # Handles Gemini API calls and response generation
â”œâ”€â”€ app.py            # Streamlit frontend (UI + app logic)
```

---

## ğŸ”§ Technologies Used

* **Python 3**
* **Streamlit**
* **Google Gemini API**
* **PyNgrok** (for exposing the app publicly)
* **Google Colab / VS Code (optional)**

---

## ğŸ”‘ Setup Instructions

### 1ï¸âƒ£ Install Dependencies

```
!pip install google-generativeai streamlit pyngrok
```

### 2ï¸âƒ£ Set up Gemini API Key

Create an environment variable:

```
export GEMINI_API_KEY="your_api_key_here"
```

or on **Windows**:

```
set GEMINI_API_KEY=your_api_key_here
```

---

## â–¶ï¸ Running the App

### **Local Machine**

```
streamlit run app.py
```

### **From Colab Using Ngrok**

Run:

```
!streamlit run app.py & npx localtunnel --port 8501
```

Or using PyNgrok:

```
from pyngrok import ngrok
public_url = ngrok.connect(8501)
```

---

## ğŸ’¡ How It Works

### **1. User enters a message in Streamlit**

Streamlit passes it to `generate_response()` inside `chatbot.py`.

### **2. Gemini receives the prompt**

The Google LLM processes the chat history + new message.

### **3. Streamlit displays the model output**

Chat history updates and repeats.

